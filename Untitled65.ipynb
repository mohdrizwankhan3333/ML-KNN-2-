{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcfc4502-d782-4619-a2ba-f1cdb265eb2c",
   "metadata": {},
   "source": [
    "Q1. What is the main difference between the Euclidean distance metric and the Manhattan distance metric in KNN? How might this difference affect the performance of a KNN classifier or regressor?\n",
    "\n",
    "The Euclidean distance measures the straight-line distance between points, while Manhattan distance sums the absolute differences along each dimension. Euclidean distance may be more sensitive to large differences, affecting performance in high-dimensional spaces.\n",
    "\n",
    "Q2. How do you choose the optimal value of k for a KNN classifier or regressor? What techniques can be used to determine the optimal k value?\n",
    "\n",
    "The optimal K value is chosen using cross-validation, where the dataset is split into training and validation sets to test various K values and select the one minimizing error.\n",
    "\n",
    "Q3. How does the choice of distance metric affect the performance of a KNN classifier or regressor? In what situations might you choose one distance metric over the other?\n",
    "\n",
    "The distance metric affects neighbor determination. Euclidean is preferred for continuous, isotropic spaces, while Manhattan suits grid-like, city-block layouts or high-dimensional data with irrelevant features.\n",
    "\n",
    "Q4. What are some common hyperparameters in KNN classifiers and regressors, and how do they affect the performance of the model? How might you go about tuning these hyperparameters to improve model performance?\n",
    "\n",
    "Common hyperparameters include K (number of neighbors) and distance metric. They influence bias-variance trade-off and model sensitivity. Tuning involves cross-validation, grid search, or random search.\n",
    "\n",
    "Q5. How does the size of the training set affect the performance of a KNN classifier or regressor? What techniques can be used to optimize the size of the training set?\n",
    "\n",
    "Larger training sets improve accuracy but increase computation. Techniques like feature selection, data sampling, or dimensionality reduction (PCA) can optimize size and efficiency.\n",
    "\n",
    "Q6. What are some potential drawbacks of using KNN as a classifier or regressor? How might you overcome these drawbacks to improve the performance of the model?**\n",
    "\n",
    "Drawbacks include computational cost, sensitivity to irrelevant features, and high-dimensional data issues. Overcome by scaling features, reducing dimensions, using efficient data structures, or ensemble methods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
